{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tqHQxrlGexIT"
      },
      "outputs": [],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYWN8EGqebGO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.nn import functional as F\n",
        "import pandas as pd\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogKlogyU2bpN"
      },
      "outputs": [],
      "source": [
        "clip.available_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHjxEjxueqq4"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMVLE4mUe8K1"
      },
      "outputs": [],
      "source": [
        "model, CliPPreprocess = clip.load('ViT-B/32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxlj0JQcZL2g"
      },
      "outputs": [],
      "source": [
        "csv_path = '/content/drive/MyDrive/results.csv'\n",
        "dataset = pd.read_csv(csv_path, delimiter = '|')\n",
        "dataset.columns = ['image_name', 'comment_number', 'comment']\n",
        "dataset['comment_number'] = dataset['comment_number'].str.strip()\n",
        "dataset = dataset.drop([19995,19996,19997,19998,19999])\n",
        "dataset['comment_number'] = pd.to_numeric(dataset['comment_number'])\n",
        "dataset['comment'] = dataset['comment'].str.strip()\n",
        "\n",
        "captions = dataset['comment'].tolist()\n",
        "\n",
        "words = []\n",
        "for caption in captions:\n",
        "  words.extend(caption.split())\n",
        "words = [word.lower() for word in words]\n",
        "\n",
        "from collections import Counter\n",
        "word_counts = Counter(words)\n",
        "\n",
        "vocab = []\n",
        "for word,freq in word_counts.items():\n",
        "  if freq >= 3:\n",
        "    vocab.append(word)\n",
        "vocab = ['<PAD>', '<UNK>','<START>','<END>'] + vocab\n",
        "\n",
        "word_to_index = {word:index for index,word in enumerate(vocab)}\n",
        "index_to_word = {index:word for index,word in enumerate(vocab)}\n",
        "\n",
        "def caption_to_indices(captions, word_to_index):\n",
        "    # iterate over each caption\n",
        "    captions_numerical = []\n",
        "    for caption in captions:\n",
        "        caption_indices = []\n",
        "        caption_indices.append(word_to_index['<START>'])\n",
        "        caption_indices.extend([word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in caption.lower().split()])\n",
        "        caption_indices.append(word_to_index['<END>'])\n",
        "        captions_numerical.append(caption_indices)\n",
        "    return captions_numerical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJz2bOJMgaF5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "CLIPOutputs_path = '/content/drive/MyDrive/image_representations.npy'\n",
        "image_representations = np.load(CLIPOutputs_path, allow_pickle = True)\n",
        "\n",
        "padded_captions_path = '/content/drive/MyDrive/Flickr30kCaptions.npy'\n",
        "padded_captions = np.load(padded_captions_path, allow_pickle = True)\n",
        "\n",
        "tensors = [t.squeeze() for t in image_representations]\n",
        "image_representations = torch.stack(tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjZ80RJogyWV"
      },
      "outputs": [],
      "source": [
        "padded_captions = tf.reshape(padded_captions, [31782,5,84])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "b1VbSnh6pF0C"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import io\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/flickr30k_images.zip'\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    image_files = [name for name in zip_ref.namelist() if name.endswith('.jpg')]\n",
        "\n",
        "    # Create an empty NumPy array to store the images\n",
        "    images = np.empty(len(image_files), dtype=object)\n",
        "\n",
        "    # Loop over the image file names\n",
        "    for i, file_name in enumerate(image_files):\n",
        "        # Read the image data from the zip file\n",
        "        with zip_ref.open(file_name) as image_file:\n",
        "            # Open the image using PIL\n",
        "            image = Image.open(io.BytesIO(image_file.read()))\n",
        "\n",
        "            # Store the processed image in the array\n",
        "            images[i] = image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmENhIBHpjnF"
      },
      "outputs": [],
      "source": [
        "# Remove image 4000\n",
        "images = np.delete(images, 3999, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IOXIUXPbqKb4"
      },
      "outputs": [],
      "source": [
        "representations = []\n",
        "i=0\n",
        "# Process each image and generate representations\n",
        "for image in images:\n",
        "    # Preprocess the image using the preprocess function from clip.load\n",
        "    preprocessed_image = CliPPreprocess(image).unsqueeze(0).to('cuda')\n",
        "    # Generate the image representation\n",
        "    with torch.no_grad():\n",
        "        representation = model.encode_image(preprocessed_image).float()\n",
        "\n",
        "    # Append the representation to the list\n",
        "    representations.append(representation)\n",
        "\n",
        "# Convert the list of representations to a NumPy array\n",
        "representations_np = np.array(representations)\n",
        "\n",
        "# Define the path to save the representations\n",
        "representations_path = '/content/drive/MyDrive/CLIP_outputs.npy'  # Modify as desired\n",
        "\n",
        "# Save the representations as a .npy file\n",
        "np.save(representations_path, representations_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhT77tUCyJN8"
      },
      "outputs": [],
      "source": [
        "numerical_captions = caption_to_indices(captions,word_to_index)\n",
        "\n",
        "numerical_captions_tensors = [torch.tensor(caption) for caption in numerical_captions]\n",
        "padded_captions = pad_sequence(numerical_captions_tensors, batch_first = True)\n",
        "\n",
        "np.save('/content/drive/MyDrive/Flickr30kCaptions.npy', padded_captions)\n",
        "\n",
        "#important variables are now padded_captions (tensor) and representations_np (numpy array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB7y5yPxEwPC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}